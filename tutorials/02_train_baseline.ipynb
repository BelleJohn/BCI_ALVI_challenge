{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Baseline\n",
    "\n",
    "This notebook shows how to train the baseline model for this competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lutetia/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(1, os.path.realpath(os.path.pardir))\n",
    "\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "from utils.train import TrainConfig, run_train_model\n",
    "from utils.augmentations import get_default_transform\n",
    "from utils import creating_dataset\n",
    "\n",
    "# this is the implementation of the custom baseline model\n",
    "from utils import hvatnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define trainer configuration\n",
    "\n",
    "The `TrainConfig` class is used to train the baseline model - have a look at the parameters it has!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_config = TrainConfig(exp_name='test_2_run_fedya', p_augs=0.3, batch_size=64, eval_interval=150, num_workers=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting val datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Getting train datasets\n",
      "Number of moves: 72 | Dataset: fedya_tropin_standart_elbow_left\n",
      "Reorder this dataset fedya_tropin_standart_elbow_left True\n",
      "Number of moves: 70 | Dataset: valery_first_standart_elbow_left\n",
      "Reorder this dataset valery_first_standart_elbow_left True\n",
      "Number of moves: 135 | Dataset: alex_kovalev_standart_elbow_left\n",
      "Reorder this dataset alex_kovalev_standart_elbow_left True\n",
      "Number of moves: 72 | Dataset: anna_makarova_standart_elbow_left\n",
      "Reorder this dataset anna_makarova_standart_elbow_left True\n",
      "Number of moves: 62 | Dataset: artem_snailbox_standart_elbow_left\n",
      "Reorder this dataset artem_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: matthew_antonov_standart_elbow_left\n",
      "Reorder this dataset matthew_antonov_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_left\n",
      "Reorder this dataset misha_korobok_standart_elbow_left True\n",
      "Number of moves: 71 | Dataset: nikita_snailbox_standart_elbow_left\n",
      "Reorder this dataset nikita_snailbox_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: petya_chizhov_standart_elbow_left\n",
      "Reorder this dataset petya_chizhov_standart_elbow_left True\n",
      "Number of moves: 12 | Dataset: polina_maksimova_standart_elbow_left\n",
      "Reorder this dataset polina_maksimova_standart_elbow_left True\n",
      "Number of moves: 144 | Dataset: sema_duplin_standart_elbow_left\n",
      "Reorder this dataset sema_duplin_standart_elbow_left True\n",
      "Number of moves: 136 | Dataset: alex_kovalev_standart_elbow_right\n",
      "Number of moves: 69 | Dataset: andrew_snailbox_standart_elbow_right\n",
      "Number of moves: 132 | Dataset: anna_makarova_standart_elbow_right\n",
      "Number of moves: 67 | Dataset: artem_snailbox_standart_elbow_right\n",
      "Number of moves: 68 | Dataset: matthew_antonov_standart_elbow_right\n",
      "Number of moves: 72 | Dataset: matvey_gorbenko_standart_elbow_right\n",
      "Number of moves: 144 | Dataset: misha_korobok_standart_elbow_right\n",
      "Number of moves: 55 | Dataset: nikita_snailbox_standart_elbow_right\n",
      "Number of moves: 142 | Dataset: petya_chizhov_standart_elbow_right\n",
      "Number of moves: 54 | Dataset: polina_maksimova_standart_elbow_right\n",
      "Number of moves: 139 | Dataset: sema_duplin_standart_elbow_right\n",
      "Number of trainining sessions: 22\n",
      "Number of validation sessions: 1\n",
      "Size of the input (8, 256) || Size of the output (20, 32)\n"
     ]
    }
   ],
   "source": [
    "# DATA_PATH = r\"F:\\Dropbox (Personal)\\BCII\\BCI Challenges\\2024 ALVI EMG Decoding\\dataset_v2_blocks\\dataset_v2_blocks\"\n",
    "DATA_PATH = \"/media/lutetia/Extreme SSD/EMG_Yun/bci-initiative-alvi-hci-challenge/dataset_v2_blocks/dataset_v2_blocks\"\n",
    "\n",
    "def count_parameters(model): \n",
    "    n_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    n_total = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total: {n_total/1e6:.2f}M, Trainable: {n_trainable/1e6:.2f}M\")\n",
    "    return n_total, n_trainable\n",
    "\n",
    "\n",
    "    \n",
    "## Data preparation\n",
    "transform = get_default_transform(train_config.p_augs)\n",
    "data_paths = dict(datasets=[DATA_PATH],\n",
    "                    hand_type = ['left', 'right'], # [left, 'right']\n",
    "                    human_type = ['health', 'amputant'], # [amputant, 'health']\n",
    "                    test_dataset_list = ['fedya_tropin_standart_elbow_left'])\n",
    "data_config = creating_dataset.DataConfig(**data_paths)\n",
    "train_dataset, test_dataset = creating_dataset.get_datasets(data_config, transform=transform)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "As you can see below, the model has a number of hyperparameters specifying its architecture and parameters. These are the parameters used to generate the baseline predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 4606052\n",
      "Total: 4.63M, Trainable: 4.63M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4628892, 4628892)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config = hvatnet.Config(n_electrodes=8, n_channels_out=20,\n",
    "                            n_res_blocks=3, n_blocks_per_layer=3,\n",
    "                            n_filters=128, kernel_size=3, \n",
    "                            strides=(2, 2, 2), dilation=2, \n",
    "                            small_strides = (2, 2))\n",
    "model = hvatnet.HVATNetv3(model_config)\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the predictions are downsampled at 25Hz from the data originally recorded at 200Hz. The `hvatnet` model used here, automatically and correctly downsamples the data during predictions. Make sure that your model's oputput is also downsampled!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (8, 256), Y shape: (20, 32)\n",
      "Predictions shape: (20, 32)\n"
     ]
    }
   ],
   "source": [
    "X, Y = train_dataset[0]\n",
    "print(f\"X shape: {X.shape}, Y shape: {Y.shape}\")\n",
    "\n",
    "Y_hat = model(torch.tensor(X).unsqueeze(0)).squeeze().detach().numpy()\n",
    "\n",
    "print(f\"Predictions shape: {Y_hat.shape}\")\n",
    "\n",
    "assert Y.shape == Y_hat.shape, \"Predictions have the wrong shape!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code trains the baseline model using training code defined in `utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed initialization of scheduler\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 150: 0.18418948352336884\n",
      "val loss: 0.18413858115673065\n",
      "saved model:  step_150_loss_0.1841.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 300: 0.16503773629665375\n",
      "val loss: 0.18900351226329803\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 450: 0.14719733595848083\n",
      "val loss: 0.2148147076368332\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 600: 0.15040133893489838\n",
      "val loss: 0.20316249132156372\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 750: 0.1171407625079155\n",
      "val loss: 0.2205028384923935\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 900: 0.13439209759235382\n",
      "val loss: 0.15777459740638733\n",
      "saved model:  step_900_loss_0.1578.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1050: 0.1528666466474533\n",
      "val loss: 0.19171355664730072\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1200: 0.14473019540309906\n",
      "val loss: 0.1551574319601059\n",
      "saved model:  step_1200_loss_0.1552.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1350: 0.11374398320913315\n",
      "val loss: 0.16537021100521088\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1500: 0.12297165393829346\n",
      "val loss: 0.17791007459163666\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1650: 0.12343708425760269\n",
      "val loss: 0.15439292788505554\n",
      "saved model:  step_1650_loss_0.1544.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1800: 0.11122602224349976\n",
      "val loss: 0.15253397822380066\n",
      "saved model:  step_1800_loss_0.1525.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 1950: 0.11258599907159805\n",
      "val loss: 0.13714198768138885\n",
      "saved model:  step_1950_loss_0.1371.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2100: 0.4354894757270813\n",
      "val loss: 0.4797495901584625\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2250: 0.14470849931240082\n",
      "val loss: 0.26657506823539734\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2400: 0.13939419388771057\n",
      "val loss: 0.15863755345344543\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2550: 0.12111661583185196\n",
      "val loss: 0.14320127665996552\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2700: 0.14039264619350433\n",
      "val loss: 0.1665765792131424\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 2850: 0.12529373168945312\n",
      "val loss: 0.15216095745563507\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3000: 0.12954415380954742\n",
      "val loss: 0.17379440367221832\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3150: 0.1037176251411438\n",
      "val loss: 0.14776276051998138\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3300: 0.13111281394958496\n",
      "val loss: 0.15070635080337524\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3450: 0.11257970333099365\n",
      "val loss: 0.14167520403862\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3600: 0.1176423653960228\n",
      "val loss: 0.13472825288772583\n",
      "saved model:  step_3600_loss_0.1347.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3750: 0.10315974801778793\n",
      "val loss: 0.14483577013015747\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 3900: 0.11393143981695175\n",
      "val loss: 0.15960554778575897\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4050: 0.10162347555160522\n",
      "val loss: 0.1568966954946518\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4200: 0.11137255281209946\n",
      "val loss: 0.14182120561599731\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4350: 0.11408193409442902\n",
      "val loss: 0.1470431536436081\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4500: 0.10758604854345322\n",
      "val loss: 0.14534077048301697\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4650: 0.12068092077970505\n",
      "val loss: 0.1555057018995285\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4800: 0.1186637207865715\n",
      "val loss: 0.14817702770233154\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 4950: 0.14342746138572693\n",
      "val loss: 0.13846082985401154\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5100: 0.09722286462783813\n",
      "val loss: 0.1342082917690277\n",
      "saved model:  step_5100_loss_0.1342.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5250: 0.09117277711629868\n",
      "val loss: 0.14150555431842804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5400: 0.11200153827667236\n",
      "val loss: 0.1339019536972046\n",
      "saved model:  step_5400_loss_0.1339.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5550: 0.12179367989301682\n",
      "val loss: 0.14406058192253113\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5700: 0.11925041675567627\n",
      "val loss: 0.12972703576087952\n",
      "saved model:  step_5700_loss_0.1297.safetensors\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 5850: 0.09487636387348175\n",
      "val loss: 0.15419887006282806\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6000: 0.11261212080717087\n",
      "val loss: 0.1378306895494461\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6150: 0.09609055519104004\n",
      "val loss: 0.13986782729625702\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6300: 0.11505910009145737\n",
      "val loss: 0.14224426448345184\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6450: 0.11166196316480637\n",
      "val loss: 0.15076783299446106\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6600: 0.09461087733507156\n",
      "val loss: 0.13358452916145325\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6750: 0.09875454753637314\n",
      "val loss: 0.15522663295269012\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 6900: 0.08367481082677841\n",
      "val loss: 0.14424629509449005\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7050: 0.1134902760386467\n",
      "val loss: 0.13936278223991394\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7200: 0.0957590639591217\n",
      "val loss: 0.1446642130613327\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7350: 0.09400618821382523\n",
      "val loss: 0.14154790341854095\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7500: 0.12179102003574371\n",
      "val loss: 0.14326539635658264\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7650: 0.11370295286178589\n",
      "val loss: 0.13703645765781403\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7800: 0.09831532090902328\n",
      "val loss: 0.1452011913061142\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 7950: 0.10530581325292587\n",
      "val loss: 0.13989408314228058\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8100: 0.08432137221097946\n",
      "val loss: 0.149953231215477\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8250: 0.09861218184232712\n",
      "val loss: 0.14039652049541473\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8400: 0.09803628921508789\n",
      "val loss: 0.1377062350511551\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8550: 0.0945747047662735\n",
      "val loss: 0.13733477890491486\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8700: 0.09910160303115845\n",
      "val loss: 0.14203880727291107\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 8850: 0.11063213646411896\n",
      "val loss: 0.14269617199897766\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9000: 0.11024739593267441\n",
      "val loss: 0.13738490641117096\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9150: 0.0872446671128273\n",
      "val loss: 0.14888426661491394\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9300: 0.09563503414392471\n",
      "val loss: 0.1463063359260559\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9450: 0.1291588842868805\n",
      "val loss: 0.13883745670318604\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9600: 0.11957617104053497\n",
      "val loss: 0.13442514836788177\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9750: 0.07691016048192978\n",
      "val loss: 0.1496986746788025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 9900: 0.11907026916742325\n",
      "val loss: 0.13967040181159973\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10050: 0.10882284492254257\n",
      "val loss: 0.1444786638021469\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10200: 0.08669286966323853\n",
      "val loss: 0.14055851101875305\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10350: 0.10610581934452057\n",
      "val loss: 0.14957284927368164\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10500: 0.11426114290952682\n",
      "val loss: 0.14868447184562683\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10650: 0.09701939672231674\n",
      "val loss: 0.1378946304321289\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10800: 0.09327705949544907\n",
      "val loss: 0.13979358971118927\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 10950: 0.11234380304813385\n",
      "val loss: 0.15656614303588867\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11100: 0.12087880820035934\n",
      "val loss: 0.1365044265985489\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11250: 0.1075502410531044\n",
      "val loss: 0.14676041901111603\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11400: 0.2240847647190094\n",
      "val loss: 0.18986034393310547\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11550: 0.20453257858753204\n",
      "val loss: 0.18188348412513733\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11700: 0.20746056735515594\n",
      "val loss: 0.18173132836818695\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 11850: 0.18934178352355957\n",
      "val loss: 0.1823962777853012\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12000: 0.1944788098335266\n",
      "val loss: 0.18227384984493256\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12150: 0.21210956573486328\n",
      "val loss: 0.18295015394687653\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12300: 0.21137705445289612\n",
      "val loss: 0.18411673605442047\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12450: 0.21078713238239288\n",
      "val loss: 0.18646970391273499\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12600: 0.20221972465515137\n",
      "val loss: 0.18532423675060272\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12750: 0.20172953605651855\n",
      "val loss: 0.1817341446876526\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 12900: 0.19730232656002045\n",
      "val loss: 0.18180827796459198\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13050: 0.19229568541049957\n",
      "val loss: 0.18263256549835205\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13200: 0.2074546366930008\n",
      "val loss: 0.1821316033601761\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13350: 0.19387872517108917\n",
      "val loss: 0.1820269674062729\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13500: 0.21447785198688507\n",
      "val loss: 0.18269549310207367\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13650: 0.207423597574234\n",
      "val loss: 0.1819983273744583\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13800: 0.19487576186656952\n",
      "val loss: 0.18254707753658295\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 13950: 0.2065494805574417\n",
      "val loss: 0.1822677105665207\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14100: 0.22914770245552063\n",
      "val loss: 0.1833835393190384\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14250: 0.21487276256084442\n",
      "val loss: 0.1882982850074768\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14400: 0.1836378425359726\n",
      "val loss: 0.18234296143054962\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14550: 0.20516113936901093\n",
      "val loss: 0.1868976354598999\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14700: 0.21670594811439514\n",
      "val loss: 0.1935664415359497\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 14850: 0.20701134204864502\n",
      "val loss: 0.18499325215816498\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15000: 0.19841334223747253\n",
      "val loss: 0.1858060657978058\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15150: 0.20995616912841797\n",
      "val loss: 0.1947641372680664\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15300: 0.22036094963550568\n",
      "val loss: 0.195173442363739\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15450: 0.20034854114055634\n",
      "val loss: 0.18259012699127197\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15600: 0.18560628592967987\n",
      "val loss: 0.18236865103244781\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15750: 0.21051612496376038\n",
      "val loss: 0.18228505551815033\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 15900: 0.2639164328575134\n",
      "val loss: 0.1879747211933136\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16050: 0.19366858899593353\n",
      "val loss: 0.1874871701002121\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16200: 0.18440861999988556\n",
      "val loss: 0.20043329894542694\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16350: 0.19821293652057648\n",
      "val loss: 0.1827727109193802\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16500: 0.22142468392848969\n",
      "val loss: 0.19128599762916565\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16650: 0.22040782868862152\n",
      "val loss: 0.19611363112926483\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16800: 0.20454345643520355\n",
      "val loss: 0.18357966840267181\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 16950: 0.2243567258119583\n",
      "val loss: 0.19234342873096466\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17100: 0.20138859748840332\n",
      "val loss: 0.1881258338689804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17250: 0.21829429268836975\n",
      "val loss: 0.18367494642734528\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17400: 0.21793042123317719\n",
      "val loss: 0.18447022140026093\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17550: 0.20687444508075714\n",
      "val loss: 0.182233989238739\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17700: 0.22613616287708282\n",
      "val loss: 0.18552576005458832\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 17850: 0.21456556022167206\n",
      "val loss: 0.1821165829896927\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18000: 0.18177294731140137\n",
      "val loss: 0.1848699152469635\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18150: 0.19848978519439697\n",
      "val loss: 0.18231414258480072\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18300: 0.1894720047712326\n",
      "val loss: 0.18280598521232605\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18450: 0.2158670723438263\n",
      "val loss: 0.18628853559494019\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18600: 0.22281010448932648\n",
      "val loss: 0.22143661975860596\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18750: 0.228316530585289\n",
      "val loss: 0.18839840590953827\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 18900: 0.19182589650154114\n",
      "val loss: 0.18545356392860413\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19050: 0.2117229700088501\n",
      "val loss: 0.18488362431526184\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19200: 0.2348286211490631\n",
      "val loss: 0.20234526693820953\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19350: 0.21373067796230316\n",
      "val loss: 0.18426521122455597\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19500: 0.23063316941261292\n",
      "val loss: 0.18258036673069\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19650: 0.20709215104579926\n",
      "val loss: 0.1824488490819931\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19800: 0.21156921982765198\n",
      "val loss: 0.19304350018501282\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 19950: 0.2053453028202057\n",
      "val loss: 0.18596123158931732\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20100: 0.23800711333751678\n",
      "val loss: 0.2034175843000412\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20250: 0.18713253736495972\n",
      "val loss: 0.18398278951644897\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20400: 0.1935400813817978\n",
      "val loss: 0.1836560219526291\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20550: 0.22344017028808594\n",
      "val loss: 0.18436473608016968\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20700: 0.20292511582374573\n",
      "val loss: 0.18275922536849976\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 20850: 0.18582920730113983\n",
      "val loss: 0.1837141513824463\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21000: 0.20225460827350616\n",
      "val loss: 0.18277110159397125\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21150: 0.1790979653596878\n",
      "val loss: 0.18453502655029297\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21300: 0.19734671711921692\n",
      "val loss: 0.1825856864452362\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21450: 0.21536944806575775\n",
      "val loss: 0.18308387696743011\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21600: 0.20053069293498993\n",
      "val loss: 0.18337154388427734\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21750: 0.1807340830564499\n",
      "val loss: 0.1835818886756897\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 21900: 0.1958494782447815\n",
      "val loss: 0.18411901593208313\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22050: 0.19842815399169922\n",
      "val loss: 0.18459129333496094\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22200: 0.21406221389770508\n",
      "val loss: 0.19100719690322876\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22350: 0.20670071244239807\n",
      "val loss: 0.18854743242263794\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22500: 0.2024536430835724\n",
      "val loss: 0.1847992241382599\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22650: 0.21091365814208984\n",
      "val loss: 0.1835554987192154\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22800: 0.20780478417873383\n",
      "val loss: 0.18226367235183716\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 22950: 0.21098513901233673\n",
      "val loss: 0.18249468505382538\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23100: 0.2042427808046341\n",
      "val loss: 0.18343882262706757\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23250: 0.2215656340122223\n",
      "val loss: 0.1832006573677063\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23400: 0.21376386284828186\n",
      "val loss: 0.18356576561927795\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23550: 0.20573997497558594\n",
      "val loss: 0.1915937215089798\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23700: 0.20704932510852814\n",
      "val loss: 0.18364165723323822\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 23850: 0.19142575562000275\n",
      "val loss: 0.1821257770061493\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24000: 0.20460902154445648\n",
      "val loss: 0.18220451474189758\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24150: 0.19927261769771576\n",
      "val loss: 0.1822994351387024\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24300: 0.2128136157989502\n",
      "val loss: 0.1820296198129654\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24450: 0.18413610756397247\n",
      "val loss: 0.18214024603366852\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24600: 0.20722220838069916\n",
      "val loss: 0.18233080208301544\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24750: 0.20152516663074493\n",
      "val loss: 0.18260809779167175\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 24900: 0.18792584538459778\n",
      "val loss: 0.18220682442188263\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25050: 0.19433198869228363\n",
      "val loss: 0.18431447446346283\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25200: 0.2149665206670761\n",
      "val loss: 0.18663927912712097\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25350: 0.21170563995838165\n",
      "val loss: 0.18494103848934174\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25500: 0.18709003925323486\n",
      "val loss: 0.18399129807949066\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25650: 0.20701752603054047\n",
      "val loss: 0.19077032804489136\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25800: 0.20841769874095917\n",
      "val loss: 0.18353118002414703\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 25950: 0.23051683604717255\n",
      "val loss: 0.18444345891475677\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26100: 0.17698226869106293\n",
      "val loss: 0.1843082755804062\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26250: 0.1972663700580597\n",
      "val loss: 0.18238534033298492\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26400: 0.2217559814453125\n",
      "val loss: 0.18233931064605713\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26550: 0.21276696026325226\n",
      "val loss: 0.19220978021621704\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26700: 0.20325401425361633\n",
      "val loss: 0.18426981568336487\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 26850: 0.2237834483385086\n",
      "val loss: 0.18710604310035706\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27000: 0.21826277673244476\n",
      "val loss: 0.18342971801757812\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27150: 0.1782594919204712\n",
      "val loss: 0.1826791763305664\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27300: 0.20844034850597382\n",
      "val loss: 0.19479110836982727\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27450: 0.19254957139492035\n",
      "val loss: 0.18308615684509277\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27600: 0.22313006222248077\n",
      "val loss: 0.1839853823184967\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27750: 0.20131011307239532\n",
      "val loss: 0.1838962435722351\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 27900: 0.18872451782226562\n",
      "val loss: 0.18392011523246765\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28050: 0.21238569915294647\n",
      "val loss: 0.18638060986995697\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28200: 0.2018698751926422\n",
      "val loss: 0.18924671411514282\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28350: 0.20948898792266846\n",
      "val loss: 0.18275322020053864\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28500: 0.19862523674964905\n",
      "val loss: 0.18405655026435852\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28650: 0.21498608589172363\n",
      "val loss: 0.18870492279529572\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28800: 0.2315738946199417\n",
      "val loss: 0.18434198200702667\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 28950: 0.22559504210948944\n",
      "val loss: 0.18296605348587036\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29100: 0.21486817300319672\n",
      "val loss: 0.18422919511795044\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29250: 0.19892778992652893\n",
      "val loss: 0.18490934371948242\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29400: 0.22548119723796844\n",
      "val loss: 0.19083380699157715\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29550: 0.20637336373329163\n",
      "val loss: 0.18393786251544952\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29700: 0.2252047061920166\n",
      "val loss: 0.18211936950683594\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 29850: 0.21523188054561615\n",
      "val loss: 0.18273968994617462\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30000: 0.18619313836097717\n",
      "val loss: 0.1852031797170639\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30150: 0.20458488166332245\n",
      "val loss: 0.18331213295459747\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30300: 0.1819659024477005\n",
      "val loss: 0.18340939283370972\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30450: 0.2002369910478592\n",
      "val loss: 0.18202883005142212\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30600: 0.19899693131446838\n",
      "val loss: 0.18296881020069122\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30750: 0.1956816464662552\n",
      "val loss: 0.18188932538032532\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 30900: 0.19468505680561066\n",
      "val loss: 0.18290168046951294\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31050: 0.19154813885688782\n",
      "val loss: 0.1822010576725006\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31200: 0.21205201745033264\n",
      "val loss: 0.18425707519054413\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31350: 0.20147226750850677\n",
      "val loss: 0.18231873214244843\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31500: 0.2120886892080307\n",
      "val loss: 0.18210232257843018\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31650: 0.20135770738124847\n",
      "val loss: 0.1820473074913025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31800: 0.1987237185239792\n",
      "val loss: 0.18205519020557404\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 31950: 0.1987556666135788\n",
      "val loss: 0.18347413837909698\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32100: 0.2090887874364853\n",
      "val loss: 0.18248485028743744\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32250: 0.21227024495601654\n",
      "val loss: 0.18213364481925964\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32400: 0.1978016197681427\n",
      "val loss: 0.18191877007484436\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32550: 0.205399751663208\n",
      "val loss: 0.18200090527534485\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32700: 0.197309672832489\n",
      "val loss: 0.18198949098587036\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 32850: 0.20356176793575287\n",
      "val loss: 0.18191972374916077\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33000: 0.19310025870800018\n",
      "val loss: 0.18206655979156494\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33150: 0.19701950252056122\n",
      "val loss: 0.18196117877960205\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33300: 0.17200298607349396\n",
      "val loss: 0.18195459246635437\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33450: 0.16943486034870148\n",
      "val loss: 0.1821114420890808\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33600: 0.18893666565418243\n",
      "val loss: 0.18210415542125702\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33750: 0.19094723463058472\n",
      "val loss: 0.18203191459178925\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 33900: 0.22138483822345734\n",
      "val loss: 0.18200595676898956\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34050: 0.1868157833814621\n",
      "val loss: 0.1827012598514557\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34200: 0.1878858357667923\n",
      "val loss: 0.1837242990732193\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34350: 0.18830402195453644\n",
      "val loss: 0.18215101957321167\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34500: 0.1878187656402588\n",
      "val loss: 0.18341787159442902\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34650: 0.19384537637233734\n",
      "val loss: 0.18473704159259796\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34800: 0.18304581940174103\n",
      "val loss: 0.19511665403842926\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 34950: 0.20612071454524994\n",
      "val loss: 0.18541017174720764\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35100: 0.1962658315896988\n",
      "val loss: 0.18565526604652405\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35250: 0.202925443649292\n",
      "val loss: 0.18253488838672638\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35400: 0.19773364067077637\n",
      "val loss: 0.18452715873718262\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35550: 0.21031026542186737\n",
      "val loss: 0.18316154181957245\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35700: 0.1955786943435669\n",
      "val loss: 0.1863395720720291\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 35850: 0.20794086158275604\n",
      "val loss: 0.18246454000473022\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36000: 0.19428136944770813\n",
      "val loss: 0.18345478177070618\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36150: 0.18661175668239594\n",
      "val loss: 0.18280763924121857\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36300: 0.21003496646881104\n",
      "val loss: 0.18264974653720856\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36450: 0.19873134791851044\n",
      "val loss: 0.18203726410865784\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36600: 0.20496301352977753\n",
      "val loss: 0.18201875686645508\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36750: 0.20578674972057343\n",
      "val loss: 0.18218888342380524\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 36900: 0.17587928473949432\n",
      "val loss: 0.18195270001888275\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37050: 0.19925843179225922\n",
      "val loss: 0.1820085048675537\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37200: 0.20362357795238495\n",
      "val loss: 0.181878924369812\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37350: 0.19071905314922333\n",
      "val loss: 0.18201635777950287\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37500: 0.19345718622207642\n",
      "val loss: 0.18278121948242188\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37650: 0.19660650193691254\n",
      "val loss: 0.1818898767232895\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37800: 0.1928221732378006\n",
      "val loss: 0.1822018325328827\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 37950: 0.20132990181446075\n",
      "val loss: 0.1819777935743332\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38100: 0.19642631709575653\n",
      "val loss: 0.1820492148399353\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38250: 0.20306740701198578\n",
      "val loss: 0.18230387568473816\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38400: 0.1990204155445099\n",
      "val loss: 0.1822492629289627\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38550: 0.19416683912277222\n",
      "val loss: 0.18245315551757812\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38700: 0.20482929050922394\n",
      "val loss: 0.18266165256500244\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 38850: 0.21621417999267578\n",
      "val loss: 0.18205036222934723\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39000: 0.19302690029144287\n",
      "val loss: 0.1819852739572525\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39150: 0.2082356959581375\n",
      "val loss: 0.1823197603225708\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39300: 0.19137941300868988\n",
      "val loss: 0.18197262287139893\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39450: 0.20898877084255219\n",
      "val loss: 0.18202419579029083\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39600: 0.21159105002880096\n",
      "val loss: 0.1820003092288971\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39750: 0.1927960366010666\n",
      "val loss: 0.18219855427742004\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 39900: 0.18336617946624756\n",
      "val loss: 0.1819581836462021\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40050: 0.22554440796375275\n",
      "val loss: 0.1822907030582428\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40200: 0.17833013832569122\n",
      "val loss: 0.18202342092990875\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40350: 0.19742171466350555\n",
      "val loss: 0.18223360180854797\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40500: 0.20039395987987518\n",
      "val loss: 0.1823672205209732\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40650: 0.19944587349891663\n",
      "val loss: 0.18229617178440094\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40800: 0.19509120285511017\n",
      "val loss: 0.182291179895401\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 40950: 0.1953902244567871\n",
      "val loss: 0.18219977617263794\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41100: 0.1867763251066208\n",
      "val loss: 0.18243007361888885\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41250: 0.20060567557811737\n",
      "val loss: 0.18206210434436798\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41400: 0.18804696202278137\n",
      "val loss: 0.18243569135665894\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41550: 0.20656561851501465\n",
      "val loss: 0.1821620762348175\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41700: 0.17930899560451508\n",
      "val loss: 0.18232761323451996\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 41850: 0.1979428231716156\n",
      "val loss: 0.18248827755451202\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42000: 0.18779075145721436\n",
      "val loss: 0.1822543889284134\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42150: 0.18423719704151154\n",
      "val loss: 0.18326576054096222\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42300: 0.19895778596401215\n",
      "val loss: 0.18249499797821045\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42450: 0.2079453319311142\n",
      "val loss: 0.18244194984436035\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42600: 0.2125563621520996\n",
      "val loss: 0.18229861557483673\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42750: 0.1925836056470871\n",
      "val loss: 0.18213029205799103\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 42900: 0.19916392862796783\n",
      "val loss: 0.18243244290351868\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43050: 0.18002229928970337\n",
      "val loss: 0.1822337657213211\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43200: 0.19761593639850616\n",
      "val loss: 0.1824081540107727\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43350: 0.20406828820705414\n",
      "val loss: 0.18222551047801971\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43500: 0.20017290115356445\n",
      "val loss: 0.1829897165298462\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43650: 0.19697508215904236\n",
      "val loss: 0.1822083294391632\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43800: 0.20976415276527405\n",
      "val loss: 0.183315247297287\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 43950: 0.18791326880455017\n",
      "val loss: 0.1821170598268509\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44100: 0.200642928481102\n",
      "val loss: 0.18195262551307678\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44250: 0.1991465836763382\n",
      "val loss: 0.1840440332889557\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44400: 0.18335045874118805\n",
      "val loss: 0.1820804625749588\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44550: 0.2090001404285431\n",
      "val loss: 0.1825440376996994\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44700: 0.19410620629787445\n",
      "val loss: 0.18207496404647827\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 44850: 0.1833842247724533\n",
      "val loss: 0.18234692513942719\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45000: 0.20551596581935883\n",
      "val loss: 0.18213604390621185\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45150: 0.21008186042308807\n",
      "val loss: 0.1822420358657837\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45300: 0.19122524559497833\n",
      "val loss: 0.18275809288024902\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45450: 0.20386190712451935\n",
      "val loss: 0.18253131210803986\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45600: 0.20632819831371307\n",
      "val loss: 0.18217432498931885\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45750: 0.20082533359527588\n",
      "val loss: 0.1821845918893814\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 45900: 0.20163479447364807\n",
      "val loss: 0.18233011662960052\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46050: 0.19375991821289062\n",
      "val loss: 0.18248127400875092\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46200: 0.1965498924255371\n",
      "val loss: 0.1825575828552246\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46350: 0.21335895359516144\n",
      "val loss: 0.18275569379329681\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46500: 0.2013179063796997\n",
      "val loss: 0.18333981931209564\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46650: 0.20090965926647186\n",
      "val loss: 0.18222032487392426\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46800: 0.21208079159259796\n",
      "val loss: 0.18215437233448029\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 46950: 0.20106303691864014\n",
      "val loss: 0.18223975598812103\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47100: 0.19463081657886505\n",
      "val loss: 0.18211358785629272\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47250: 0.1981423944234848\n",
      "val loss: 0.1829281598329544\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47400: 0.18818768858909607\n",
      "val loss: 0.182084858417511\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47550: 0.18263116478919983\n",
      "val loss: 0.1826772689819336\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47700: 0.19667015969753265\n",
      "val loss: 0.1824069321155548\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 47850: 0.20291700959205627\n",
      "val loss: 0.18215087056159973\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48000: 0.19967663288116455\n",
      "val loss: 0.18285417556762695\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48150: 0.18825790286064148\n",
      "val loss: 0.18243324756622314\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48300: 0.21103332936763763\n",
      "val loss: 0.18197086453437805\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48450: 0.19529037177562714\n",
      "val loss: 0.18205522000789642\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48600: 0.196042999625206\n",
      "val loss: 0.18261483311653137\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48750: 0.19949519634246826\n",
      "val loss: 0.18218283355236053\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 48900: 0.2175731211900711\n",
      "val loss: 0.18246816098690033\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49050: 0.19574527442455292\n",
      "val loss: 0.18308933079242706\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49200: 0.18915846943855286\n",
      "val loss: 0.1824522465467453\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49350: 0.20066514611244202\n",
      "val loss: 0.18241816759109497\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49500: 0.2167595624923706\n",
      "val loss: 0.1825013905763626\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49650: 0.22348614037036896\n",
      "val loss: 0.18306779861450195\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49800: 0.19306837022304535\n",
      "val loss: 0.18218888342380524\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 49950: 0.20641198754310608\n",
      "val loss: 0.18193504214286804\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50100: 0.19757001101970673\n",
      "val loss: 0.1819683462381363\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50250: 0.19603534042835236\n",
      "val loss: 0.18197551369667053\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50400: 0.18611185252666473\n",
      "val loss: 0.181923508644104\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50550: 0.21285931766033173\n",
      "val loss: 0.18199694156646729\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50700: 0.2256760150194168\n",
      "val loss: 0.18196609616279602\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 50850: 0.20646663010120392\n",
      "val loss: 0.18204393982887268\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51000: 0.20931299030780792\n",
      "val loss: 0.18203423917293549\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51150: 0.19752910733222961\n",
      "val loss: 0.18204084038734436\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51300: 0.19134803116321564\n",
      "val loss: 0.18202662467956543\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51450: 0.20393037796020508\n",
      "val loss: 0.18201084434986115\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51600: 0.1962662786245346\n",
      "val loss: 0.18199314177036285\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51750: 0.22118030488491058\n",
      "val loss: 0.18204519152641296\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 51900: 0.18263863027095795\n",
      "val loss: 0.18208858370780945\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52050: 0.22082756459712982\n",
      "val loss: 0.18194150924682617\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52200: 0.21830327808856964\n",
      "val loss: 0.18229570984840393\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52350: 0.20309744775295258\n",
      "val loss: 0.18261882662773132\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52500: 0.19699394702911377\n",
      "val loss: 0.184474378824234\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52650: 0.22569416463375092\n",
      "val loss: 0.18280738592147827\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52800: 0.2034871131181717\n",
      "val loss: 0.1826351433992386\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 52950: 0.20681889355182648\n",
      "val loss: 0.1833951771259308\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53100: 0.19541047513484955\n",
      "val loss: 0.18268221616744995\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53250: 0.20461849868297577\n",
      "val loss: 0.1818884015083313\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53400: 0.1943787783384323\n",
      "val loss: 0.1823796033859253\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53550: 0.20827503502368927\n",
      "val loss: 0.18256330490112305\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53700: 0.22152772545814514\n",
      "val loss: 0.18213382363319397\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 53850: 0.20450177788734436\n",
      "val loss: 0.18217337131500244\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54000: 0.18933208286762238\n",
      "val loss: 0.1819818615913391\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54150: 0.19167093932628632\n",
      "val loss: 0.18235497176647186\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54300: 0.1980484426021576\n",
      "val loss: 0.18361206352710724\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54450: 0.18477939069271088\n",
      "val loss: 0.182531476020813\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54600: 0.18359296023845673\n",
      "val loss: 0.18240734934806824\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54750: 0.21982227265834808\n",
      "val loss: 0.18201127648353577\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 54900: 0.21722769737243652\n",
      "val loss: 0.18245765566825867\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55050: 0.21089263260364532\n",
      "val loss: 0.18447518348693848\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55200: 0.20520491898059845\n",
      "val loss: 0.18200038373470306\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55350: 0.2134230136871338\n",
      "val loss: 0.18313026428222656\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55500: 0.19472366571426392\n",
      "val loss: 0.1825120747089386\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55650: 0.1927306354045868\n",
      "val loss: 0.1825086623430252\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55800: 0.1927367001771927\n",
      "val loss: 0.1823941469192505\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 55950: 0.2042965143918991\n",
      "val loss: 0.18247520923614502\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56100: 0.19774019718170166\n",
      "val loss: 0.1822013258934021\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56250: 0.19309313595294952\n",
      "val loss: 0.18216122686862946\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56400: 0.20235686004161835\n",
      "val loss: 0.1825646013021469\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56550: 0.21433036029338837\n",
      "val loss: 0.18222373723983765\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56700: 0.18102799355983734\n",
      "val loss: 0.18387439846992493\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 56850: 0.21111337840557098\n",
      "val loss: 0.18313388526439667\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57000: 0.2201233208179474\n",
      "val loss: 0.18289262056350708\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57150: 0.19017231464385986\n",
      "val loss: 0.1822940707206726\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57300: 0.21123278141021729\n",
      "val loss: 0.18237800896167755\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57450: 0.1886759251356125\n",
      "val loss: 0.18286781013011932\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57600: 0.20861494541168213\n",
      "val loss: 0.18204346299171448\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57750: 0.18241707980632782\n",
      "val loss: 0.1819859892129898\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 57900: 0.19022761285305023\n",
      "val loss: 0.18206787109375\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58050: 0.20644795894622803\n",
      "val loss: 0.18211735785007477\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58200: 0.1983855813741684\n",
      "val loss: 0.1820237934589386\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58350: 0.20100072026252747\n",
      "val loss: 0.18203812837600708\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58500: 0.2088640183210373\n",
      "val loss: 0.18232721090316772\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58650: 0.216486856341362\n",
      "val loss: 0.18218381702899933\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58800: 0.18812906742095947\n",
      "val loss: 0.18284159898757935\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 58950: 0.1969117522239685\n",
      "val loss: 0.18187279999256134\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59100: 0.19450615346431732\n",
      "val loss: 0.18223851919174194\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59250: 0.21204662322998047\n",
      "val loss: 0.1819378286600113\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59400: 0.1969904899597168\n",
      "val loss: 0.182823047041893\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59550: 0.20862045884132385\n",
      "val loss: 0.18591022491455078\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59700: 0.209424689412117\n",
      "val loss: 0.18242087960243225\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 59850: 0.20245066285133362\n",
      "val loss: 0.18203790485858917\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60000: 0.17460285127162933\n",
      "val loss: 0.1839546114206314\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60150: 0.2103981077671051\n",
      "val loss: 0.18232038617134094\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60300: 0.20041656494140625\n",
      "val loss: 0.18243053555488586\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60450: 0.19140858948230743\n",
      "val loss: 0.182347372174263\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60600: 0.21768318116664886\n",
      "val loss: 0.18224790692329407\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60750: 0.19806627929210663\n",
      "val loss: 0.18270717561244965\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 60900: 0.19317348301410675\n",
      "val loss: 0.18289422988891602\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61050: 0.18602176010608673\n",
      "val loss: 0.18228834867477417\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61200: 0.1847059577703476\n",
      "val loss: 0.18259580433368683\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61350: 0.19031713902950287\n",
      "val loss: 0.18204525113105774\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61500: 0.1826905757188797\n",
      "val loss: 0.18223458528518677\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61650: 0.20555122196674347\n",
      "val loss: 0.18447370827198029\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61800: 0.20841984450817108\n",
      "val loss: 0.18231311440467834\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 61950: 0.20404879748821259\n",
      "val loss: 0.18230319023132324\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62100: 0.16745978593826294\n",
      "val loss: 0.1828787922859192\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62250: 0.19382421672344208\n",
      "val loss: 0.1819068193435669\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62400: 0.2078874558210373\n",
      "val loss: 0.1822355091571808\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62550: 0.20849013328552246\n",
      "val loss: 0.1819818615913391\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62700: 0.22970807552337646\n",
      "val loss: 0.18195602297782898\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 62850: 0.1998765468597412\n",
      "val loss: 0.18236008286476135\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63000: 0.19506804645061493\n",
      "val loss: 0.1826445609331131\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63150: 0.2019716054201126\n",
      "val loss: 0.18204885721206665\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63300: 0.19229860603809357\n",
      "val loss: 0.1821952611207962\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63450: 0.21040475368499756\n",
      "val loss: 0.18223845958709717\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63600: 0.18192163109779358\n",
      "val loss: 0.18203739821910858\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63750: 0.21426372230052948\n",
      "val loss: 0.1831449419260025\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 63900: 0.20548932254314423\n",
      "val loss: 0.18365857005119324\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64050: 0.20672836899757385\n",
      "val loss: 0.18196818232536316\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64200: 0.21040980517864227\n",
      "val loss: 0.18239080905914307\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64350: 0.19306443631649017\n",
      "val loss: 0.18208104372024536\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64500: 0.20388178527355194\n",
      "val loss: 0.18245497345924377\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64650: 0.21740618348121643\n",
      "val loss: 0.1825091689825058\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64800: 0.22942209243774414\n",
      "val loss: 0.18201321363449097\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 64950: 0.1874270737171173\n",
      "val loss: 0.18253102898597717\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65100: 0.17875002324581146\n",
      "val loss: 0.18196693062782288\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65250: 0.1922162026166916\n",
      "val loss: 0.1819797307252884\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65400: 0.2037203311920166\n",
      "val loss: 0.18192270398139954\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65550: 0.21266289055347443\n",
      "val loss: 0.1819572150707245\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65700: 0.19403450191020966\n",
      "val loss: 0.1823229342699051\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 65850: 0.21572351455688477\n",
      "val loss: 0.18207025527954102\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66000: 0.22264640033245087\n",
      "val loss: 0.18264839053153992\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66150: 0.19249048829078674\n",
      "val loss: 0.18268710374832153\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66300: 0.20320165157318115\n",
      "val loss: 0.18188102543354034\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66450: 0.20255890488624573\n",
      "val loss: 0.18281042575836182\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66600: 0.20026449859142303\n",
      "val loss: 0.18251842260360718\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66750: 0.20380829274654388\n",
      "val loss: 0.18329069018363953\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 66900: 0.1905471682548523\n",
      "val loss: 0.1822841465473175\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67050: 0.21615195274353027\n",
      "val loss: 0.18245074152946472\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67200: 0.1941298544406891\n",
      "val loss: 0.18211665749549866\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67350: 0.23138435184955597\n",
      "val loss: 0.18230080604553223\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67500: 0.22365286946296692\n",
      "val loss: 0.1823883354663849\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67650: 0.1944449096918106\n",
      "val loss: 0.18774092197418213\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67800: 0.19074968993663788\n",
      "val loss: 0.18261970579624176\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 67950: 0.19854432344436646\n",
      "val loss: 0.18315839767456055\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68100: 0.20414982736110687\n",
      "val loss: 0.18259838223457336\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68250: 0.19486740231513977\n",
      "val loss: 0.18244138360023499\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68400: 0.2004847526550293\n",
      "val loss: 0.1820269674062729\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68550: 0.21256723999977112\n",
      "val loss: 0.18218068778514862\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68700: 0.20557013154029846\n",
      "val loss: 0.18201714754104614\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 68850: 0.19668319821357727\n",
      "val loss: 0.18219870328903198\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69000: 0.19422049820423126\n",
      "val loss: 0.18215367197990417\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69150: 0.18329723179340363\n",
      "val loss: 0.18253052234649658\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69300: 0.2037702351808548\n",
      "val loss: 0.18247273564338684\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69450: 0.21193742752075195\n",
      "val loss: 0.1822199523448944\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69600: 0.19534288346767426\n",
      "val loss: 0.18262115120887756\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69750: 0.18531297147274017\n",
      "val loss: 0.18232908844947815\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 69900: 0.20753243565559387\n",
      "val loss: 0.18243031203746796\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70050: 0.20529618859291077\n",
      "val loss: 0.18219570815563202\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70200: 0.18872343003749847\n",
      "val loss: 0.18191638588905334\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70350: 0.19967003166675568\n",
      "val loss: 0.18207858502864838\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70500: 0.20977692306041718\n",
      "val loss: 0.18214057385921478\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70650: 0.19062504172325134\n",
      "val loss: 0.18211133778095245\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70800: 0.2052651196718216\n",
      "val loss: 0.18216145038604736\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 70950: 0.2112208902835846\n",
      "val loss: 0.18198752403259277\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71100: 0.1875297725200653\n",
      "val loss: 0.18333862721920013\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71250: 0.195982426404953\n",
      "val loss: 0.18217900395393372\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71400: 0.2004844695329666\n",
      "val loss: 0.1827181577682495\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71550: 0.19434963166713715\n",
      "val loss: 0.18211722373962402\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71700: 0.1939554363489151\n",
      "val loss: 0.1827881783246994\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 71850: 0.2098652571439743\n",
      "val loss: 0.1821126490831375\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72000: 0.2098667472600937\n",
      "val loss: 0.18198472261428833\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72150: 0.18114109337329865\n",
      "val loss: 0.18226319551467896\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72300: 0.17547936737537384\n",
      "val loss: 0.18197734653949738\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72450: 0.17885346710681915\n",
      "val loss: 0.18198123574256897\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72600: 0.20098553597927094\n",
      "val loss: 0.18209587037563324\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72750: 0.19809584319591522\n",
      "val loss: 0.1826697140932083\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 72900: 0.21911442279815674\n",
      "val loss: 0.1828150749206543\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73050: 0.21088622510433197\n",
      "val loss: 0.18220484256744385\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73200: 0.20552027225494385\n",
      "val loss: 0.1822049915790558\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73350: 0.2097601443529129\n",
      "val loss: 0.18184863030910492\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73500: 0.19940614700317383\n",
      "val loss: 0.18820345401763916\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73650: 0.21424852311611176\n",
      "val loss: 0.18284070491790771\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73800: 0.19992923736572266\n",
      "val loss: 0.1819498986005783\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 73950: 0.22705744206905365\n",
      "val loss: 0.1823485642671585\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74100: 0.2033342570066452\n",
      "val loss: 0.18229447305202484\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74250: 0.2050030678510666\n",
      "val loss: 0.18201707303524017\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74400: 0.20454192161560059\n",
      "val loss: 0.1823590248823166\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74550: 0.18944469094276428\n",
      "val loss: 0.18225134909152985\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74700: 0.22048437595367432\n",
      "val loss: 0.18200460076332092\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 74850: 0.22432270646095276\n",
      "val loss: 0.19138634204864502\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75000: 0.21525800228118896\n",
      "val loss: 0.18291254341602325\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75150: 0.24390184879302979\n",
      "val loss: 0.184312641620636\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75300: 0.19452933967113495\n",
      "val loss: 0.18228334188461304\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75450: 0.18162822723388672\n",
      "val loss: 0.18219299614429474\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75600: 0.21401894092559814\n",
      "val loss: 0.18212707340717316\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75750: 0.20995347201824188\n",
      "val loss: 0.18179768323898315\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 75900: 0.2014915943145752\n",
      "val loss: 0.18228121101856232\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76050: 0.1768818199634552\n",
      "val loss: 0.1819077879190445\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76200: 0.19962261617183685\n",
      "val loss: 0.18212281167507172\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76350: 0.18714027106761932\n",
      "val loss: 0.1823158711194992\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76500: 0.19215236604213715\n",
      "val loss: 0.18212755024433136\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76650: 0.20001962780952454\n",
      "val loss: 0.18233329057693481\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76800: 0.19782082736492157\n",
      "val loss: 0.18202698230743408\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 76950: 0.1877591907978058\n",
      "val loss: 0.18201212584972382\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77100: 0.19542451202869415\n",
      "val loss: 0.1819997876882553\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77250: 0.1927398294210434\n",
      "val loss: 0.18210239708423615\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77400: 0.22386837005615234\n",
      "val loss: 0.1827343851327896\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77550: 0.1897793561220169\n",
      "val loss: 0.18221014738082886\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77700: 0.18443183600902557\n",
      "val loss: 0.18206483125686646\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 77850: 0.20039089024066925\n",
      "val loss: 0.18196968734264374\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78000: 0.19336239993572235\n",
      "val loss: 0.18192058801651\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78150: 0.1983940750360489\n",
      "val loss: 0.18194961547851562\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78300: 0.1952592432498932\n",
      "val loss: 0.18197280168533325\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78450: 0.1867142766714096\n",
      "val loss: 0.18196843564510345\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78600: 0.20883135497570038\n",
      "val loss: 0.18197089433670044\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78750: 0.2019757330417633\n",
      "val loss: 0.1819744110107422\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 78900: 0.20477846264839172\n",
      "val loss: 0.18198566138744354\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79050: 0.20014257729053497\n",
      "val loss: 0.18197888135910034\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79200: 0.21971145272254944\n",
      "val loss: 0.1819777488708496\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79350: 0.19390319287776947\n",
      "val loss: 0.1819782257080078\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79500: 0.20842623710632324\n",
      "val loss: 0.18197570741176605\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79650: 0.1813887655735016\n",
      "val loss: 0.1819712221622467\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79800: 0.1795908510684967\n",
      "val loss: 0.1819763034582138\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 79950: 0.20562143623828888\n",
      "val loss: 0.18195173144340515\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80100: 0.2064022570848465\n",
      "val loss: 0.18197818100452423\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80250: 0.21427865326404572\n",
      "val loss: 0.18196582794189453\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80400: 0.20044460892677307\n",
      "val loss: 0.18196624517440796\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80550: 0.19378365576267242\n",
      "val loss: 0.18196752667427063\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80700: 0.21470053493976593\n",
      "val loss: 0.18198750913143158\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 80850: 0.21137090027332306\n",
      "val loss: 0.181990846991539\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81000: 0.19584743678569794\n",
      "val loss: 0.18201372027397156\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81150: 0.1919235736131668\n",
      "val loss: 0.1820204108953476\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81300: 0.18821655213832855\n",
      "val loss: 0.18196508288383484\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81450: 0.17181919515132904\n",
      "val loss: 0.18190330266952515\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81600: 0.19485639035701752\n",
      "val loss: 0.18195977807044983\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81750: 0.1963561475276947\n",
      "val loss: 0.18193143606185913\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 81900: 0.18892844021320343\n",
      "val loss: 0.18187397718429565\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82050: 0.20672762393951416\n",
      "val loss: 0.18198278546333313\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82200: 0.19975750148296356\n",
      "val loss: 0.18198047578334808\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82350: 0.2067614048719406\n",
      "val loss: 0.18211773037910461\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82500: 0.2083926498889923\n",
      "val loss: 0.1819356232881546\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82650: 0.18868181109428406\n",
      "val loss: 0.181949183344841\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82800: 0.19936136901378632\n",
      "val loss: 0.18208913505077362\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 82950: 0.19519388675689697\n",
      "val loss: 0.18203912675380707\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83100: 0.19786231219768524\n",
      "val loss: 0.18199782073497772\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83250: 0.1971345692873001\n",
      "val loss: 0.1819876730442047\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83400: 0.19624005258083344\n",
      "val loss: 0.18197523057460785\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83550: 0.1872648447751999\n",
      "val loss: 0.18199753761291504\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83700: 0.2048041820526123\n",
      "val loss: 0.18199598789215088\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 83850: 0.20031261444091797\n",
      "val loss: 0.18198248744010925\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84000: 0.200299933552742\n",
      "val loss: 0.18193186819553375\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84150: 0.20427528023719788\n",
      "val loss: 0.1821848303079605\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84300: 0.2020784169435501\n",
      "val loss: 0.18216228485107422\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84450: 0.21163597702980042\n",
      "val loss: 0.18197691440582275\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84600: 0.1953524649143219\n",
      "val loss: 0.18195658922195435\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84750: 0.19905564188957214\n",
      "val loss: 0.1819249540567398\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 84900: 0.21519224345684052\n",
      "val loss: 0.1819872260093689\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85050: 0.20555202662944794\n",
      "val loss: 0.18204301595687866\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85200: 0.19380700588226318\n",
      "val loss: 0.18199172616004944\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85350: 0.21562349796295166\n",
      "val loss: 0.18214301764965057\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85500: 0.2138732522726059\n",
      "val loss: 0.1821608990430832\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85650: 0.20203132927417755\n",
      "val loss: 0.18250849843025208\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85800: 0.20593877136707306\n",
      "val loss: 0.18237121403217316\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 85950: 0.1856444627046585\n",
      "val loss: 0.1820060759782791\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86100: 0.1986183375120163\n",
      "val loss: 0.18222542107105255\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86250: 0.196143239736557\n",
      "val loss: 0.18189993500709534\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86400: 0.2243497371673584\n",
      "val loss: 0.18207389116287231\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86550: 0.18990996479988098\n",
      "val loss: 0.18202733993530273\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86700: 0.20621712505817413\n",
      "val loss: 0.18206174671649933\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 86850: 0.19981823861598969\n",
      "val loss: 0.18222421407699585\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87000: 0.21018047630786896\n",
      "val loss: 0.1837579756975174\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87150: 0.19504719972610474\n",
      "val loss: 0.1830872893333435\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87300: 0.23327484726905823\n",
      "val loss: 0.19740353524684906\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87450: 0.18719033896923065\n",
      "val loss: 0.18227088451385498\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87600: 0.2226349115371704\n",
      "val loss: 0.18263503909111023\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87750: 0.21739302575588226\n",
      "val loss: 0.18442299962043762\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 87900: 0.1891176402568817\n",
      "val loss: 0.18367412686347961\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88050: 0.21110840141773224\n",
      "val loss: 0.18282148241996765\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88200: 0.20227888226509094\n",
      "val loss: 0.18393583595752716\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88350: 0.21256478130817413\n",
      "val loss: 0.18192501366138458\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88500: 0.2054460346698761\n",
      "val loss: 0.18304137885570526\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88650: 0.19603393971920013\n",
      "val loss: 0.18207047879695892\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88800: 0.21219618618488312\n",
      "val loss: 0.18305188417434692\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 88950: 0.21091823279857635\n",
      "val loss: 0.1844228357076645\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89100: 0.1887347549200058\n",
      "val loss: 0.18254420161247253\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89250: 0.19049470126628876\n",
      "val loss: 0.18354743719100952\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89400: 0.1957627683877945\n",
      "val loss: 0.18221698701381683\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89550: 0.2020736038684845\n",
      "val loss: 0.1819498986005783\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89700: 0.18539689481258392\n",
      "val loss: 0.18268778920173645\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 89850: 0.18921230733394623\n",
      "val loss: 0.1823391318321228\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90000: 0.18476368486881256\n",
      "val loss: 0.18268050253391266\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90150: 0.22542591392993927\n",
      "val loss: 0.1829363852739334\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90300: 0.22629939019680023\n",
      "val loss: 0.1835743486881256\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90450: 0.20795989036560059\n",
      "val loss: 0.1824871450662613\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90600: 0.18843860924243927\n",
      "val loss: 0.18255674839019775\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90750: 0.19396288692951202\n",
      "val loss: 0.18232229351997375\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 90900: 0.20399685204029083\n",
      "val loss: 0.18252959847450256\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91050: 0.17874090373516083\n",
      "val loss: 0.18230703473091125\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91200: 0.19240932166576385\n",
      "val loss: 0.18206267058849335\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91350: 0.18267184495925903\n",
      "val loss: 0.18314294517040253\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91500: 0.20026496052742004\n",
      "val loss: 0.18208621442317963\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91650: 0.19226811826229095\n",
      "val loss: 0.18228456377983093\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91800: 0.1768491119146347\n",
      "val loss: 0.1820860654115677\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 91950: 0.2109946310520172\n",
      "val loss: 0.18224237859249115\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92100: 0.20291586220264435\n",
      "val loss: 0.18261851370334625\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92250: 0.19185082614421844\n",
      "val loss: 0.1820376217365265\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92400: 0.21433329582214355\n",
      "val loss: 0.18199995160102844\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92550: 0.1932193785905838\n",
      "val loss: 0.18209439516067505\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92700: 0.18436411023139954\n",
      "val loss: 0.18212491273880005\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 92850: 0.2018393576145172\n",
      "val loss: 0.18220359086990356\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93000: 0.21094463765621185\n",
      "val loss: 0.18222084641456604\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93150: 0.19989268481731415\n",
      "val loss: 0.1823575347661972\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93300: 0.19428037106990814\n",
      "val loss: 0.18249478936195374\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93450: 0.20233599841594696\n",
      "val loss: 0.18235711753368378\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93600: 0.2247992753982544\n",
      "val loss: 0.18318744003772736\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93750: 0.19540777802467346\n",
      "val loss: 0.18264442682266235\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 93900: 0.18493469059467316\n",
      "val loss: 0.1822347491979599\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94050: 0.19650042057037354\n",
      "val loss: 0.1823195368051529\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94200: 0.2165488451719284\n",
      "val loss: 0.18258197605609894\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94350: 0.17984946072101593\n",
      "val loss: 0.18258531391620636\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94500: 0.198928102850914\n",
      "val loss: 0.18216195702552795\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94650: 0.19106541574001312\n",
      "val loss: 0.1826026290655136\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94800: 0.2088509052991867\n",
      "val loss: 0.18230219185352325\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 94950: 0.20606186985969543\n",
      "val loss: 0.18290340900421143\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95100: 0.1855231374502182\n",
      "val loss: 0.18286894261837006\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95250: 0.19850274920463562\n",
      "val loss: 0.18274910748004913\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95400: 0.19680774211883545\n",
      "val loss: 0.18204890191555023\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95550: 0.19154003262519836\n",
      "val loss: 0.1823214441537857\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95700: 0.20117413997650146\n",
      "val loss: 0.18222461640834808\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 95850: 0.219060018658638\n",
      "val loss: 0.18238845467567444\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96000: 0.1781621277332306\n",
      "val loss: 0.18222036957740784\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96150: 0.20451174676418304\n",
      "val loss: 0.1820719689130783\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96300: 0.18034811317920685\n",
      "val loss: 0.18214651942253113\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96450: 0.21289542317390442\n",
      "val loss: 0.1842477023601532\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96600: 0.1881185621023178\n",
      "val loss: 0.18231676518917084\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96750: 0.21682298183441162\n",
      "val loss: 0.18248453736305237\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 96900: 0.2094724178314209\n",
      "val loss: 0.18209098279476166\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97050: 0.1930285543203354\n",
      "val loss: 0.1825181543827057\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97200: 0.20061969757080078\n",
      "val loss: 0.18206092715263367\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97350: 0.20447444915771484\n",
      "val loss: 0.18395614624023438\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97500: 0.20388245582580566\n",
      "val loss: 0.1821586638689041\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97650: 0.20465455949306488\n",
      "val loss: 0.18210826814174652\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97800: 0.181597501039505\n",
      "val loss: 0.18389402329921722\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 97950: 0.19372402131557465\n",
      "val loss: 0.1826309859752655\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98100: 0.19837942719459534\n",
      "val loss: 0.1821126937866211\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98250: 0.22408537566661835\n",
      "val loss: 0.18328857421875\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98400: 0.21935008466243744\n",
      "val loss: 0.18206852674484253\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98550: 0.19703122973442078\n",
      "val loss: 0.1821117103099823\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98700: 0.1942841112613678\n",
      "val loss: 0.18213358521461487\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 98850: 0.19592803716659546\n",
      "val loss: 0.18315915763378143\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99000: 0.20348243415355682\n",
      "val loss: 0.18225468695163727\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99150: 0.1906646490097046\n",
      "val loss: 0.18309104442596436\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99300: 0.19784776866436005\n",
      "val loss: 0.1823149174451828\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99450: 0.1868593394756317\n",
      "val loss: 0.18215519189834595\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99600: 0.19678997993469238\n",
      "val loss: 0.18215006589889526\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99750: 0.19088731706142426\n",
      "val loss: 0.1819513887166977\n",
      "\n",
      "\n",
      "******************************************************************************************************************************************************\n",
      "\n",
      "overall_steps 99900: 0.20667552947998047\n",
      "val loss: 0.1820417195558548\n",
      "\n",
      "\n",
      "*****************************************************************************************************Complete training\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.12972703576087952"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "import numpy as np\n",
    "from scipy.ndimage import uniform_filter1d, gaussian_filter1d\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Post-processing functions\n",
    "def moving_average(data, window_size, axis=0):\n",
    "    return uniform_filter1d(data, size=window_size, axis=axis, mode='reflect')\n",
    "\n",
    "def savitzky_golay_filter(data, window_size, poly_order, axis=0):\n",
    "    return savgol_filter(data, window_size, poly_order, axis=axis)\n",
    "\n",
    "def gaussian_smoothing(data, sigma, axis=0):\n",
    "    return gaussian_filter1d(data, sigma, axis=axis)\n",
    "\n",
    "def example_post_process(data):\n",
    "    return None\n",
    "    # return moving_average(data, window_size=3,axis=-1) # window_size=3-15\n",
    "    # return savitzky_golay_filter(data, window_size=3, poly_order=2) #window_size=3-15, poly_order=2-5\n",
    "    # return gaussian_smoothing(data, sigma=1) #sigma=0.1-2\n",
    "\n",
    "\n",
    "run_train_model(model, (train_dataset, test_dataset), train_config, device) #post_process=example_post_process #trial=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "omen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
